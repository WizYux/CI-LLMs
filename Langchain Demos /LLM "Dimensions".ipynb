{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "from langchain.llms import OpenAI, Cohere, Ollama\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI, ChatOllama\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "from ipywidgets import fixed, interactive, interact_manual, FloatSlider\n",
    "\n",
    "# API Keys Here\n",
    "os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'\n",
    "os.environ['COHERE_API_KEY'] = 'COHERE_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How should I tour France?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Prompt Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Parameters of LLMs\"\n",
    "\n",
    "# Two Types of LLMs: text / chat\n",
    "# We integrate companies / models\n",
    "# As one drop-down menu here for convenience\n",
    "\n",
    "# 1) Models:\n",
    "chat_models = [\"OpenAI gpt-4\", \"OpenAI gpt-4-0314\", \"OpenAI gpt-4-0613\", \"META llama2\"]\n",
    "text_models = [\"OpenAI text-davinci-003\", \"Cohere command\", \"Cohere command-light\", \"META llama2\"]\n",
    "\n",
    "# 2) Temperature\n",
    "temperature = FloatSlider(min=0.0, max=1.0, step=0.01, value=0.0)\n",
    "\n",
    "# 3) Response Length\n",
    "length = FloatSlider(min=50.0, max=150.0, step=1, value=0.0)\n",
    "\n",
    "\"\"\"\n",
    "ASPECTS OF LANGUAGE BELOW (SUBJECT TO INTERPRETATION)\n",
    "\"\"\"\n",
    "\n",
    "# 4) Diction\n",
    "diction = [\"neutral\", \"formal\", \"informal\"]\n",
    "\n",
    "# 5) Tone\n",
    "tone = [\"neutral\", \"excited\", \"nonchalant\"]\n",
    "\n",
    "# 6) Confidence\n",
    "confidence = [\"neutral\", \"confident\", \"reserved\"]\n",
    "\n",
    "# 7) Comprehension\n",
    "comprehension = [\"neutral\", \"simple\", \"complex\"]\n",
    "\n",
    "# 8) Appeal\n",
    "appeal = [\"neutral\", \"emotional\", \"logical\"]\n",
    "\n",
    "# 9) Diversity\n",
    "diversity = [\"neutral\", \"diverse\", \"narrow\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loader(name: str, temperature: float):\n",
    "\n",
    "    match name.split()[0]:\n",
    "        case \"OpenAI\":\n",
    "            model = OpenAI(model_name = name.split()[1], temperature = temperature)\n",
    "        case \"Cohere\":\n",
    "            model = Cohere(model = name.split()[1], temperature = temperature)\n",
    "        case \"META\":\n",
    "            model = Ollama(model = name.split()[1], temperature = temperature)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_output(query, text_models, temperature, length, diction, tone, confidence, comprehension, appeal, diversity):\n",
    "\n",
    "    max_char = 100\n",
    "\n",
    "    model = model_loader(text_models, temperature)\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "\n",
    "        input_variables = [\"query\", \"length\", \"diction\", \"tone\", \"confidence\", \"comprehension\", \"appeal\", \"diversity\"],\n",
    "        \n",
    "        template = \"\"\"Directions: your tone should be {tone}. Your diction should be {diction}. \n",
    "        You should speak in a {confidence} manner. Your output should be {comprehension}. \n",
    "        Your appeal should be {appeal}. Your range of topics discussed should be {diversity}.\n",
    "        Please keep your response to strictly within {length} words. \n",
    "        \\n\\n Question: {query} \\n\\n Your response here: \"\"\",\n",
    "\n",
    "    )\n",
    "\n",
    "    prompt = prompt_template.format(query = query, length = length, diction = diction, tone = tone, confidence = confidence, \\\n",
    "                                                      comprehension = comprehension, appeal = appeal, diversity = diversity)\n",
    "\n",
    "    lines = [line + \"\\n\" for line in textwrap.wrap(model(prompt), width = max_char, break_long_words=False)]\n",
    "    \n",
    "    output = \"\".join(lines)\n",
    "    print(output + \"\\n\")\n",
    "    print(f\"Word Count: {len(output.split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429653cea3b84664b336b275fde728b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='text_models', options=('OpenAI text-davinci-003', 'Cohere command'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.manual_output(query, text_models, temperature, length, diction, tone, confidence, comprehension, appeal, diversity)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(manual_output, query = fixed(query), text_models = text_models, temperature = temperature, length = length, diction = diction, tone = tone, confidence = confidence, comprehension = comprehension, appeal = appeal, diversity = diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"LLM as Optimizers\": —> LLM for Prompt Generation[?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_in = PromptTemplate(\n",
    "\n",
    "    input_variables = [\"length\", \"diction\", \"tone\", \"confidence\", \"comprehension\", \"appeal\", \"diversity\"],\n",
    "    \n",
    "    template = \"\"\"You are a helpful AI bot that provides instructions on how other LLMs should generate output.\n",
    "    Please generate a set of instructions that tells other LLMs to do the following: 1) {diction} diction \n",
    "    2) {tone} tone 3) {confidence} manner 4) {comprehension} comprehension 5) {appeal} appeal \n",
    "    6) {diversity} topics discussed 7) {length} words or less. Your output should only include \n",
    "    the related instructions and nothing else. \\n\\n Instructions:\"\"\",\n",
    "\n",
    ")\n",
    "\n",
    "prompt_template_out = PromptTemplate(\n",
    "\n",
    "    input_variables = [\"LLM_prompt\", \"query\"],\n",
    "    \n",
    "    template = \"\"\"Please generate your output by following the instructions below.\n",
    "        Directions: {LLM_prompt} \\n\\n Question: {query} \\n\\n Your response here: \"\"\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_output(query, text_models, temperature, length, diction, tone, confidence, comprehension, appeal, diversity):\n",
    "\n",
    "    max_char = 100\n",
    "\n",
    "    model = model_loader(text_models, temperature)\n",
    "    generator = OpenAI(model_name = \"text-davinci-003\", temperature = 1.0)\n",
    "\n",
    "    prompt_in = prompt_template_in.format(length = length, diction = diction, tone = tone, confidence = confidence, \\\n",
    "                                                      comprehension = comprehension, appeal = appeal, diversity = diversity)\n",
    "    \n",
    "    LLM_prompt = generator(prompt_in)\n",
    "    \n",
    "    prompt_out = prompt_template_out.format(LLM_prompt = LLM_prompt, query = query)\n",
    "\n",
    "    lines = [line + \"\\n\" for line in textwrap.wrap(model(prompt_out), width = max_char, break_long_words=False)]\n",
    "    print(LLM_prompt + \"\\n\")\n",
    "\n",
    "    output = \"\".join(lines)\n",
    "    print(output + \"\\n\")\n",
    "    print(f\"Word Count: {len(output.split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d722651f3174fd38288769002c05083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='text_models', options=('OpenAI text-davinci-003', 'Cohere command'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.LLM_output(query, text_models, temperature, length, diction, tone, confidence, comprehension, appeal, diversity)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(LLM_output, query = fixed(query), text_models = text_models, temperature = temperature, length = length, diction = diction, tone = tone, confidence = confidence, comprehension = comprehension, appeal = appeal, diversity = diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Templating —> LLM Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(text_models, temperature, length, diction, tone, confidence, comprehension, appeal, diversity):\n",
    "\n",
    "    return text_models, temperature, length, diction, tone, confidence, comprehension, appeal, diversity\n",
    "\n",
    "def output_mod(text_models, temperature, length, diction, tone, confidence, comprehension, appeal, diversity):\n",
    "\n",
    "    parameters = interactive(identity, query = fixed(query), text_models = text_models, \\\n",
    "                        temperature = temperature, length = length, diction = diction, tone = tone, \\\n",
    "                        confidence = confidence, comprehension = comprehension, appeal = appeal, diversity = diversity)\n",
    "    \n",
    "    display(parameters)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_out2 = PromptTemplate(\n",
    "\n",
    "    input_variables = [\"LLM_prompt\", \"query\", \"examples\"],\n",
    "    \n",
    "    template = \"\"\"Please generate {examples} outputs by following the directions below.\n",
    "        Each output will be graded from a scale of 1 to 10, with 1 being the worst and \n",
    "        10 being the best, according to how well it follows the directions. You should \n",
    "        definitely not score the responses yourself and should only generate {examples} \n",
    "        responses, each of which should independently follow the directions below. \\n\\n\n",
    "        Directions: {LLM_prompt} \\n\\n Question: {query} \\n\\n Your responses here: \"\"\",\n",
    "\n",
    ")\n",
    "\n",
    "prompt_template_grader = PromptTemplate(\n",
    "\n",
    "    input_variables = [\"LLM_prompt\", \"query\", \"examples\", \"output\"],\n",
    "\n",
    "    template = \"\"\"You are a harsh AI grader that is given {examples} different responses to the question {query}. \n",
    "        For each response, you will assign an integer from 1 to 10 only (not /10), with 1 being the worst and 10 being \n",
    "        the best,according to how closely the response follows the directions. You should deduct 1 to 2 points \n",
    "        for each listed direction that the generated response did not follow. You should give a response 0 points if \n",
    "        it did not effectively address the query {query}. Your output should resemble the following: \n",
    "        \\n 1: 8 \\n 2: 6 \\n\\n Directions: {LLM_prompt} \\n\\n Response: {output} \\n\\n Scores: \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_examples(params: list, examples: int, query: str):\n",
    "\n",
    "    text_models, temperature, length, diction, tone, confidence, comprehension, appeal, diversity = [param for param in params]\n",
    "\n",
    "    model = model_loader(text_models, temperature)\n",
    "    generator = OpenAI(model_name = \"text-davinci-003\", temperature = 1.0)\n",
    "\n",
    "    prompt_in = prompt_template_in.format(length = length, diction = diction, tone = tone, confidence = confidence, \\\n",
    "                                                      comprehension = comprehension, appeal = appeal, diversity = diversity)\n",
    "    \n",
    "    LLM_prompt = generator(prompt_in)\n",
    "    \n",
    "    prompt_out = prompt_template_out2.format(LLM_prompt = LLM_prompt, query = query, examples = examples)\n",
    "    few_shot_responses = model(prompt_out)\n",
    "\n",
    "    grader = OpenAI(model_name = \"text-davinci-003\", temperature = 0.0)\n",
    "\n",
    "    prompt_grader = prompt_template_grader.format(LLM_prompt = LLM_prompt, query = query, examples = examples, output = few_shot_examples)\n",
    "    few_shot_scores = grader(prompt_grader)\n",
    "\n",
    "    return LLM_prompt, few_shot_responses, few_shot_scores, model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_output(params: tuple, examples: int, query: str):\n",
    "    \n",
    "    max_char = 100\n",
    "\n",
    "    LLM_prompt, few_shot_examples, few_shot_scores, model = rate_examples(params, examples, query)\n",
    "\n",
    "    few_shot_examples = list(filter(None, few_shot_examples.splitlines()))\n",
    "    few_shot_scores = list(filter(None, few_shot_scores.splitlines()))\n",
    "    few_shot_scores = [item.strip() for item in [re.sub(r'^.*? ', ' ', score.strip()) for score in few_shot_scores]]\n",
    "\n",
    "    template_examples = []\n",
    "\n",
    "    for i in range(examples):\n",
    "        template_examples.append({\"Output\": few_shot_examples[i], \"Score\": few_shot_scores[i]})\n",
    "    \n",
    "    template = \"\"\"\n",
    "    Response: {Output}\n",
    "    Score: {Score}  \n",
    "    \"\"\"\n",
    "\n",
    "    prefix = \"\"\"In the following, the AI follows the directions below to generate a response to the query. \n",
    "    Each response is associated with a score from 1 (worst) to 10 (best). \\n\\n Here is the question: {query} \n",
    "    \\n\\n Here are the directions: {LLM_prompt} Here are some examples: \\n\\n \"\"\"\n",
    "\n",
    "    suffix = \"\"\"Please generate a response to the query by following the directions. Aim for the highest score possible.\n",
    "    \\n\\n Directions: {LLM_prompt} \\n\\n Query: {query} \\n\\n Your response here (do not include score or word count): \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"Output\", \"Score\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    few_shot_template = FewShotPromptTemplate(\n",
    "    examples=template_examples, \n",
    "    example_prompt=prompt_template,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"LLM_prompt\", \"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "\n",
    "    few_shot_prompt = few_shot_template.format(LLM_prompt = LLM_prompt, query = query)\n",
    "\n",
    "    print(few_shot_prompt)\n",
    "\n",
    "    output = model(few_shot_prompt)\n",
    "\n",
    "    lines = [line + \"\\n\" for line in textwrap.wrap(model(few_shot_prompt), width = max_char, break_long_words=False)]\n",
    "    # print(LLM_prompt + \"\\n\")\n",
    "\n",
    "    output = \"\".join(lines)\n",
    "    print(output + \"\\n\")\n",
    "    print(f\"Word Count: {len(output.split())}\")\n",
    "\n",
    "    # return model(few_shot_prompt), few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c30b0589b5402fbc0dd8dbd6be6f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='text_models', options=('OpenAI text-davinci-003', 'Cohere command'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = output_mod(text_models, temperature, length, diction, tone, confidence, comprehension, appeal, diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('OpenAI text-davinci-003', 0.45, 85.0, 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral')\n"
     ]
    }
   ],
   "source": [
    "print(params.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the following, the AI follows the directions below to generate a response to the query. \n",
      "    Each response is associated with a score from 1 (worst) to 10 (best). \n",
      "\n",
      " Here is the question: How should I tour France? \n",
      "    \n",
      "\n",
      " Here are the directions:  \n",
      "1. Use a neutral diction that is simple and concise. \n",
      "2. Maintain a neutral, even tone when speaking. \n",
      "3. Speak in a professional manner. \n",
      "4. Comprehend any conversations in an impartial manner. \n",
      "5. Appeal to the listener in a balanced way. \n",
      "6. Discuss neutral topics relevant to the context. \n",
      "7. Once complete, limit the output to 85 words. Here are some examples: \n",
      "\n",
      " \n",
      "\n",
      "    Response: 1. France is a beautiful country with many different areas to explore. Start by researching the different regions and cities to determine which ones you'd like to visit. Consider the type of activities you'd like to do and the attractions you'd like to see. Plan your route accordingly and make sure to book accommodation in advance. Once you have your plan, make sure to enjoy the journey! \n",
      "    Score: 9  \n",
      "    \n",
      "\n",
      "    Response: 2. Touring France is an amazing experience. There are plenty of places to visit and activities to do. Consider the different regions and cities that you'd like to explore and plan your route accordingly. Make sure to book accommodation in advance and research the attractions and activities that interest you. Enjoy the journey! \n",
      "    Score: 8  \n",
      "    \n",
      "\n",
      "    Response: 3. When touring France, there are many different regions and cities to explore. Research the different areas to determine which ones you'd like to visit. Consider the type of activities you'd like to do and the attractions you'd like to see. Plan your route and book your accommodation in advance. When you're ready, enjoy the journey!\n",
      "    Score: 10  \n",
      "    \n",
      "Please generate a response to the query by following the directions. Aim for the highest score possible.\n",
      "    \n",
      "\n",
      " Directions:  \n",
      "1. Use a neutral diction that is simple and concise. \n",
      "2. Maintain a neutral, even tone when speaking. \n",
      "3. Speak in a professional manner. \n",
      "4. Comprehend any conversations in an impartial manner. \n",
      "5. Appeal to the listener in a balanced way. \n",
      "6. Discuss neutral topics relevant to the context. \n",
      "7. Once complete, limit the output to 85 words. \n",
      "\n",
      " Query: How should I tour France? \n",
      "\n",
      " Your response here (do not include score or word count): \n",
      " When touring France, there are many different regions and cities to explore. Research the different\n",
      "areas to determine which ones you'd like to visit. Consider the type of activities you'd like to do\n",
      "and the attractions you'd like to see. Plan your route and book your accommodation in advance. Make\n",
      "sure to enjoy the journey and all the sights and sounds of France!\n",
      "\n",
      "\n",
      "Word Count: 63\n"
     ]
    }
   ],
   "source": [
    "LLM_output(params.result, 3, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
